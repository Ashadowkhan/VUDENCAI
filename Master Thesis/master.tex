\documentclass[
	a4paper,
	pagesize,
	pdftex,
	12pt,
	twoside, % + BCOR darunter: für doppelseitigen Druck aktivieren, sonst beide deaktivieren
	BCOR=5mm, % Dicke der Bindung berücksichtigen (Copyshop fragen, wie viel das ist)
	ngerman,
	fleqn,
	final,
	]{scrartcl}
\usepackage{ucs}
\usepackage{bm}
\usepackage[utf8x]{inputenc} % Eingabekodierung: UTF-8
\usepackage{fixltx2e} % Schickere Ausgabe
\usepackage[T1]{fontenc} % ordentliche Trennung
\usepackage[ngerman]{babel}
\usepackage{lmodern} % ordentliche Schriften
\usepackage{enumitem}
\usepackage[unicode=true]{hyperref}
\usepackage{listings}
\lstset{
	basicstyle=\ttfamily,
	columns=fullflexible,
	frame=single,
	breaklines=true,
}
\usepackage{setspace,graphicx,tikz,tabularx} % für Elemente der Titelseite

\usepackage[sort, numbers]{natbib}
\usepackage[draft=false,babel,tracking=true,kerning=true,spacing=true]{microtype} % optischer Randausgleich etc.

\newcommand{\myparagraph}[1]{\paragraph{#1}\mbox{}\\}



\begin{document}

% Beispielhafte Nutzung der Vorlage für die Titelseite (bitte anpassen):
\input{Institutsvorlage}
\titel{Automated Security Vulnerability Detection in Source Code Using Deep Learning On Github Data} % Titel der Arbeit
\typ{Masterarbeit} % Typ der Arbeit:  Diplomarbeit, Masterarbeit, Bachelorarbeit
\grad{Master of Science (M. Sc.)} % erreichter Akademischer Grad
% z.B.: Master of Science (M. Sc.), Master of Education (M. Ed.), Bachelor of Science (B. Sc.), Bachelor of Arts (B. A.), Diplominformatikerin
\autor{Laura Wartschinski} % Autor der Arbeit, mit Vor- und Nachname
\gebdatum{11.05.1994} % Geburtsdatum des Autors
\gebort{Eisenach} % Geburtsort des Autors
\gutachter{Prof. Dr. Lars Grunske}{Prof. Dr. XYZ} % Erst- und Zweitgutachter der Arbeit
\mitverteidigung % entfernen, falls keine Verteidigung erfolgt
\makeTitel
% Hier folgt die eigentliche Arbeit (bei doppelseitigem Druck auf einem neuen Blatt):
\tableofcontents
\newpage

\section{Introduction}
Computer Software is indispensible for modern human life, and software security issues can cause severe harm. Many different approaches to detect and prevent software vulnerabilities have been proposed, with recent works using machine learning and data mining techniques (for a survey, see \citep{Ghaffarian.2017}). This work aims to develop a tool capable of learning typical vulnerability patterns form a big software repository (github) and automatically extracting patterns, that can then by applied to find new vulnerabilities in code. To achieve this, the first step is to gather a large dataset of python code with and without security vulnerabilities, and then use deep learning techniques to interprete the code. The applicability and use of the developed tool with be evaluated by measuring the true and false positive rate as well as the acceptance rate of the proposed changes in real projects. 

\section{Motivation}\label{Motivation}
Modern society would not work the way it does without computer programs. Software has become a neccessity for several aspects of society, including healthcare, energy, transportation, public safety, education, entertainment and many more. However, to create safe, reliable and securesoftware is by no means a trivial task. Oversights and errors made by software architects and engineers can easily cause vulnerabilities - and the consequences are severe. An exploit like the ransomware WannaCry will, for example, shut down hospitals and transportation systems and cause hundreds of millions of dollars in damage~\cite{DanGoodin.2017}. Just a small flaw in the code, sometimes only spanning one or a few lines, can be enough to cause a severe vulnerability and make the system an easy target for attacks~\citep{Yamaguchi.2012}. To give another example, the famous Heartbleed bug, a severe vulnerability in the OpenSSL cryptographic library that affected billions of internet users, could have been prevented with two more lines of code \citep{Durumeric.2014}. Cyberattacks are an increasing threat to governments, businesses and consumers~\cite{Dam.2017}, costing an estimated 400 billion dollars a year~\cite{Losses.2014}. At the same time, the number of vulnerabilities officially registered in the Commen Vulnerabilities and Exposures (CVE) was approximately 6500 in 2016~\cite{CVE}. %TODO aktuellere Zahl
\newline
%the number of vulnerabilities registered in the Common Vulnerabilities and Exposures (CVE) was approximately 4,600 in 2010, and grew to approximately 6,500 in 2016 quoting CVE, http://cve.mitre.org/.

Any code change in a project can possibly alter the attack surface or introduce a security vulnerability ~\cite{Morrison.2015}. Discovering vulnerabilities is a tedious, time-consuming process that requires substantial knowledge in the area of security \cite{Yamaguchi.2011}, mostly because modern software systems are becoming increasingly complex and interconnected, and vulnerabilities can lie in very innocous looking segments of code~\cite{Pang.2015, Li.2018}. Vulnerabilities often result from violating unspoken, implicit programming rules that can be hard to keep track of~\cite{Li.2005}. Software tester attempting to detect vulnerabilities need to know not only the language and architecture of the software in question, but also malicious strategies and mindsets, reasoning like an attacker~\cite{Pang.2015}. And since there is no systematic way that covers all possible flaws, coming up with identifying features that define vulnerabilities is largely an art, depending strongly on individual expertise, and it is hard for developers stay ahead in the game~\cite{Rolim.2018,Li.2018}.\\
To make matters worse, many security vulnerabilities don't affect the the typical functionalities of a system can therefore stay undetected for a long time. Those 'hidden vulnerabilities' can have disastrous effects, since they are usually only discovered long after the bug has been introduced, giving attackers ample time to exploit the vulnerability \cite{Wijayasekara.2012,Ma.2017,Russell.2018}. Due to the prevalence of open source software, code re-use (e.g. github forks) etc., vulnerable code can propagate quickyl from one project to the next. 
And even if a nearly unlimited time is spent searching for more vulnerabilities, defenders can never be sure they have caught everything, while at the same time, an attacker only needs to find one single useful vulnerability to be able to cause harm and e.g. cause a program to crash or expose sensitive information.\\

The quality of defined features (and therefore the power of a vulnerability detection system) varies strongly with the individual creating them. Therefore, the result must be improved by asking multiple experts and combining their results, which is an even more tedious work~\cite{Li.2018}. Instead, it is desireable to reduce the need for human labour as much as possible. A study funded by IBM measured the adoption rate and the effect of automation (mostly referring to machine learning and artificial intelligence) in detecting cyber attacks ~\cite{IBMNewsRoom.}. They came to the conclusion that various automation techniques help significantly with preventing exploits, but that around 77\% of organizations use automation only moderately, insignificantly or not at all, pointing to a strong potential of further improvement. It is therefore an important goal to support human experts with tool that can reduce or eliminate the need for the most tedious and error-prone tasks in detecting vulnerabilities.

Some tools are already used to support developers, for example by helping with prioritization and testing and reducing the time spent on the task~\cite{Dam.2017}. The dominant approach so far has been a formal one, which aims to model programming langauges with mathematical structures and prove certain properties~\cite{Allamanis.2018}, for example about code coverage and data flows. With symbolic execution, input data is replaced with symbolic values and analyzed as it changes over the control flow of the program. Static analysis tools analyse source code without executing it, following a rule-based approach. Some examples for widely used tools include FindBugs~\cite{Hovemeyer.2004,Hovemeyer.2005} for Java, Splint~\cite{Evans.2002} and Flawfinder~\cite{Wheeler.2006} for C, and PyT~\cite{Micheelsen.2016} (python taint) for Python. They play an important role, but they are not enough to significantly reduce the prevalence of vulnerabilities. Many tools used today rely on static analysis and suffer from a high false positive rate, which hinders their adoption ~\cite{Liu.2018} and leads to various problems described below, in addition to the fact that their criteria still have to be defined by experts and can therefore lag behind recent developments.\\

%der folgende Absatz könnte etwas länger sein
A new approach is emerging with methods based on data analysis and machine learning. It is driven by the goal to design a vulnerability detection system without relying on subjective expert opinions and without suffering from high false negative or high false positive rates~\cite{Li.2018}. The core idea is to derive patterns from large amounts of data, using a machine learning algorithm to discover vulnerability features and classifying code accordingly. This means that vulnerability detection can be automated, and possibly also appliedy ver eary in the software lifecycle, which can significantly reduce the costs of finding and fixing errors~\cite{Dam.2017}.


\section{Background}

\subsection{Discovering vulnerabilities}

The problem of software security vulnerabilities is a crucial one. The NIST defines a vulnerability as 'Weakness in an information system, system security procedures, internal controls, or implementation that could be exploited or triggered by a threat source'~\cite{NISTComputerSecurityRessourceCenter.}; similarly, they are described as '...specific flaws or oversights
in a piece of software that allow attackers to do something malicious: expose or alter sensitive information, disrupt or destroy a system, or take control of a computer system or program' in a standard handbook on software security assessment~\cite{Dowd.2006}.
Vulnerabilities can be viewed as a subsect of defects ~\cite{Morrison.2015}, although they occur much less frequently. For example, ~\cite{Shin.2013} report that roughly 21\% of files in Mozilla Firefox have defects, but only 3\% have some kind of security vulnerability. 
Much effort has been invested in different techniques to find and fix vulnerabilities before they can be exploited, and recently, Machine Learning approaches have been applied to this cause as well. Partly following the terminology of \citep{Ghaffarian.2017}, the following section contains an overview of those approaches. 
%Approaches can be devided into 1) Vulnerability Prediction models based on software metrics, 2) anomaly detection approaches, and 3) vulnerable code pattern recognition. 

%Taint Analysis and so should be described. Check out Vulnerability Extrapolation Assisted Discovery ...

\subsubsection{Static Analysis}
In static code analysis, the source code is analyzed without executing it, using generalization and abstract rules \cite{Ghaffarian.2017}. They can identify good programming practises and potential defects, e.g. misused APIs, performance issues, deadlocks, bad practices and others \cite{Venkatasubramanyam.2014}, and in an ideal case, a static analysis tool would be \textit{sound} (missing not a single flaw), although this has to be balanced with computational limitations and the false positive rate in practise. Static code analysis tools are widely used to identify potential issues, and a wide range of commercial and open-source tools are available for use. Without any doubt, those tools have a strong positive effect on code quality \cite{Liu.2018}. \\
The success of static analysis tools depends fundamentally on the quality of the underlying patterns, abstractions or rules that are used to identify problems, and the more complex software becomes and the more creative potential attackers are, the harder it is to define patterns of secure software. To create those patterns \textit{automatically} appears to be nearly impossible \cite{Rolim.2018, Yamaguchi.2012}, and creating them manually is tedious and time consuming, especially since technology and computer systems advance quickly, and therefore keeping track of all possible kinds of vulnerabilities is hard~\cite{Ma.2017}.\\
The accuracy of static analysis tools is measured by calculating the rate of false positives (reported problems that are in fact not an issue) and false negatives (missed problems). Especially a high false positive rate in many tools is a big obstacle \cite[S.~1]{Liu.2018}, as those misclassifications cause developers to spend a large amount of time on checking each one, which makes it harder to spot the actual problems. They might even get used to ignoring the warnings in those tools, causing actual bugs and vulnerabilities to be overlooked. As \cite{Li.2005} phrases it, high false negative rates prevent a vulnerability detection system from being \textit{useful}, and high false positive make it not \textit{usable}.\\

\subsubsection{Dynamic and Hybrid Analysis}
Another option is dynamic program analysis, in which programs are executed and monitored at runtime, for example with certain test inputs. Of course, not all possible inputs and execution paths can always be covered, so dynamic analysis can be \textit{complete} (never falsely reporting a vulnerability where none exists), but it can very well miss a lot of problems. A big practical problem is that dynamic analysis requires much more computational time and ressources and is simply not possible in many circumstances, especially when analyzing larger software or software collections.
In hybrid analysis, a dynamic approach is complemented with a static analysis to identify false negatives, or vice versa, combining advantages and disadvantages of both approaches. \cite{Shar.2013}, for example, predict vulnerabilities with 90{\%} recall and 85{\%} precision on average using a hybrid approach. They work with six projects and two different vulnerability types (SQL injection and cross site scripting). The present work however aims for a bigger scope, which is not feasible with dynamic analysis.\\
%TODO Übergang



\subsection{Data Mining and Machine Learning}
Data Mining is the process of extracting knowledge from large sources of data. %DO I HAVE TO QUOTE
In the context of cyber security, software code, bugs and vulnerabilities are the data researchers are interested in. Several official bug databases, such as the Bugzilla bug database or the Common Vulnerabilities and Exposures (CVE) database ~\cite{CVE}, have been used in projects involving some form of information extraction from those sources ~\cite{Wijayasekara.2012}. But the code itself is also of great interest. Fortunately, due to the emergence of large online repositories and open source software, it is comparatively easy to gain access to large amounts of code that can then be analyzed in some way. More about the process of extracting the data can be found in section \ref{Mining-Software-Repositories}. In the recent past, more and more research has been applying Natural Language Processing techniques to software code, treating it as text (see section \ref{Natural-Hypothesis}). Thus, code features can be extracted automatically, which opens up a wide range of applications \cite{Dam.2017}.\\

Machine Learning describes computational algorithms that allow computer systems to solve problems without explicit programming. There are supervised learning techniques, which use labeled data to allow a learning system to gain insight and create a model, unsupervised learning, in which the system finds patterns and structures in the dataset without guidance, and reinforcement learning, in which the system is provided with rewards and penalties after each step to train it dynamically.\\

Machine Learning and Data Mining techniques have been used successfully in the area of security, e.g. in defining firewalls, analyzing potentially malicious code with anomaly-based detection methods. \cite{Elovici.2007} use decision trees, neural networks and bayesian networks to detect malicious code in network traffic, and \cite{Schultz.2000} use, amongst others, a multi-naive bayes model on binary files to detect malicious executables passing through a mail filter, doubling the detection rate of purely signature based methods.\\

Among the first models used for classifying code were n-gram models. They truncate the history and use the last n-1 words (or software tokens) to model code \cite{Nguyen.2013,Pang.2015}. N-gram models are able to capture the localness of software code, but they suffer from the small context problem and can't 'remember' over a long distance. More recently however, more complex methods in the realm of deep learning with neural network have been proven to be useful.\\

\subsubsection{Deep Learning and Neural Networks}\label{Deep-Learning}
A neural network consists of many simple parts that can be activated either by their environment (in case of input neurons) or through connections with other neurons. Their output is a sequence of activations, and learning in a neural network is all about finding the right connections and triggers that make the neural network exhibit a desired behavior. Deep Learning is the term applied to learning with neural networks that have a high number of layers of neurons. Deep Neural Networks have won a numerous pattern recognition competitions, achieving superhuman abilties in some domains ~\cite{Schmidhuber.2015}.\\
Deep Learning has been proven to be applicable to various software engineering tasks, including feature learning for defect prediction ~\cite{Wang.2016}, learning code suggestions ~\cite{Bhoopchand.2016}, detecting malicious code in network traffic ~\cite{Elovici.2007}, detecting malicious executables ~\cite{Schultz.2000}, detecting malicious URLs and file paths with CNNs ~\cite{Saxe.2017}, or learning features for detecting bugs and faults ~\cite{Huo.2016,Gupta.2017b}. \\
The works in ~\cite{White.2015}, ~\cite{Dam.2016b} and ~\cite{Dam.2016} demonstrated that recurrent neural networks can be effectively used to model source code, and in a later work ~\cite{White.2016}, the authors successfully model code clones with RNNs. ~\cite{Mou.2014} explored the potential for Deep Learning approaches for program analysis. They based their research on student exercises for various tasks, extracted the abstract syntax tree representation of the code and trained a tree-based CNN on several simple classification problems.\\
In summary, it can be stated that convolutional neural networks (CNNs) are well-suited for capturing structural patterns in code \cite{Dam.2016} and have been succesfully used by e.g. \cite{Huo.2016} and \cite{Russell.2018} to locate bugs in source code. Recurrent neural networks (RNNs) are especially useful for capturing long-lasting content much better than n-grams. While they require a magnitude more data than n-gram models, they are better able to understand contexts over a long distance. They can also learn a much richer notion of similarity - for example, they are able to learn that two for loops with different counting variables and different end conditions are nevertheless similar constructs, as described by \cite{Allamanis.2018}. 


\subsubsection{Reccurent Neural Networks}\label{RNN}
Standard neural networks, in which connections between nodes do not form circles, are called feedforward neural networks. Those networks take data as input, process it through the network, and transforms it into an output, e.g. a classification. The data never passes a given node of the network twice, and the network's output is not dependent on what it processed before or after the current input. Therefore, they can not take into account the context of the information they are processing. Recurrent neural networks (RNNs) introduce a mechanism that can be called a 'memory', as they use an internal state to process a sequence of inputs, and previous results and inputs have an influence on operations that are happening later. They use a feedback loop to use the outputs or previous states for the current operation to model long-term dependencies in the data and take those into account, which allows them to recognize patterns in data such as written text, sensor readings, musical notes, health data or genome sequences. As \cite{Schmidthuber.2015} puts it, RNNs can be called the 'deepest of all NNs', and they are fundamentally able to create and process memories of any input pattern.\\
How can RNNs now be leverage to classify source code and to find vulnerabilities? For the purpose of this work, it is safe to assume that a single code token (e.g. 'if') can hardly be classified as 'safe' or 'vulnerable', be it a variable, a built in object of the language or an operator. Only in the \textit{context} of the tokens that came before and after, a judgement about the qualities of the code snippet becomes feasible. Much like the other examples cited above, code tokens form a sequence that is connected by strong semantic ties, and previous declarations and operations absolutely have an influence on their successors. Just changing the initial value of a single variable can drastically alter the behavior of a complete program. Therefore, a type of recurrent neural network appears fitting to model this type of data.\\
Unfortunately, RNNs are not able to model long term dependencies. \cite{Hochreiter.1991} and \cite{Bengio.1994} show that recurrent networks with gradient descent of an error criterion become increasingly inefficient when the span between the place an information occurs first and the place it becomes relevant later grows. This is mostly because of the problem of vanishing and exploding gradients.\\
The works in ~\cite{White.2015}, ~\cite{Dam.2016b} and ~\cite{Dam.2016} demonstrated that recurrent neural networks are especially well-suited to model source code, and in a later work ~\cite{White.2016}, the authors successfully model code clones with RNNs. ~\cite{Mou.2014} explored the potential for Deep Learning approaches for program analysis. They based their research on student exercises for various tasks, extracted the abstract syntax tree representation of the code and trained a tree-based CNN on several simple classification problems. Various other applications have been found for deep learning models trained on code: for example, learning code suggestions ~\cite{Bhoopchand.2016}, detecting malicious code in network traffic ~\cite{Elovici.2007}, detecting malicious executables ~\cite{Schultz.2000}, or learning features for detecting bugs and faults ~\cite{Huo.2016,Gupta.2017b}. 

\subsubsection{Vanishing and Exploding Gradients}
To give a short summary of a more complex problem: The gradients of loss in a neural network are frequently calculated using back propagation, coming from the deeper layers to the initial one. When the gradients have to pass trough a number of layers using certain activation functions like e.g. the sigmoid activation function, which maps the weighs to the range between -1 and 1, or the hyperbolic tangent function. When training a RNN, the gradients coming from deeper layers have to pass through continuous operations of this type, since the backpropagation algorithm uses the chain rule to compute the gradients. The further they are apart from the earlier layers, the more multiplications with small number occur, making the gradient decrease exponentially. This means that the information is lost and the model can not learn from it anymore.\\
If the values output by the activation functions are larger than 1, the opposite problem occurs: the gradients get larger and eventually 'explode'. In such a case, the trained weights in the network are changed drastically and might even become so large that they cause an overflow error and become NaN, causing the whole network to break down.\\ %TODO do I need more sources here?
There are several different solutions to this problem, including gradient clipping, using different activation functions like the rectified linear activation function~\cite{Glorot.2011}, and a whole new kind of RNNs has been developed to model long term dependencies, called Long Short Term Memory networks (LSTMs).

\subsubsection{Long Short Term Memory Networks}
Sepp Hochreiter and Jürgen Schmidthuber presented the idea of LSTMs in 1997~\cite{Hochreiter.1997}. They offer a solution to the vanishing gradient problem with their network architecture, which they achieve by using a gradient-based algorithm that enforces constant error flow through the internal states, preventing both exploding and vanishing.\\
LSTMs introduce a memory cell that stores accumulated context information. The memory content can be modified by sigmoid neural net layers called 'gates': an input gate, a forget gate, and an output gate, each of which can return a value between 0 and 1. A value of 0 lets no information pass, a value of 1 means that all information is passed through. All those gates are trained as well, and influence the content of the memory cell and thereby the final output.\\
The forget gate is a sigmoid layer which has the purpose to filter out some information that shall no longer affect the cell state. For each component $C_{t-1}$ of the memory cell's state, it generates a value between 0 and 1 that determines how much of it should be kept according to $f_t = \sigma (W_f * [h_{t-1}, x_t] + b_f$, where $h_{t-1}$ corresponds to the output of the previous step, $x_t$ is the current input, and $W_f$ and $b_f$ are weights. This gates is supposed to allow information that is no longer relevant to be forgotten, so that it doesn't linger around distort the results of further steps.\\
The input gate is a multiplicative gate which is used to shield the memory contents from irrelevant inputs. Its purpse is to decide which information we want to update. It consists of two parts, both taking the resulting state of the previous step, the current input, and weights into account. One part uses a tanh gate to compute the new values: $C_t = tanh (W_c * [h_{t-1} , x_t] + b_c)$ between -1 and 1, the other one is a sigmoid gate that decides which values are even put in the memory cell: $\sigma (W_i * [h_{t-1}, x_t] + b_i$. Those are multiplied and then the result is added to the memory cell.\\
Finally, there's the output gate, which consists of a sigmoid layer which computes $o_t = \sigma (W_o * [h_{t-1}, x_t] + b_o$ to filter which values are output, that is then multiplied to the values of the memory cells after they have been modified by a tanh function to scale them between -1 and 1. The result $h_t = o_t * tanh(C_t)$ is now used as an actual part of the output of the LSTM and also fed back into the feedback loop, becoming the new $h_{t-1}$ for the next step.\\
The above description defines one typical architecture for an LSTM, however, lots of different variations have been introduced. The forget gate was not included in the very first draft of the LSTM, but developed later~\cite{Gers.1999}. Other variants of LSTMs include so called 'peephole connections', which allow the gate layers to take the current cell memory state into account \cite{Gers.2000}, and Gated Reccurent Units \cite{Cho.2014}, which merge some gates and simplify and change the design of the LSTM. \\
Long Short Term Memory networks have shown some impressive capabilities, starting in the areas of image recognition and speech tagging. LSTMs have been shown to be able to learn how to automatically tie knots in minimally invasive open heart surgery ~\cite{Mayer.2008}, to compose polyphonic music with intricate melodies and chords ~\cite{Kumar.2019}, and to analyze biological sequences to predict to which subcellular compartment a protein belongs ~\cite{Snderby.2015}. The Open AI project developed a bot based on several that can beat human players in Dota 2 ~\cite{Rodriguez.2018}, and Deep Mind's Alpha Star was able to beat professional players in Starcraft II ~\cite{Stanford.2019}, both complex and strategic real-time strategy games popular in esports. Those feats were called a 'huge milestone in advancing artificial intelligence'. Open AI also trained a LSTM to learn to give inputs to a robot hand to manipulate objects ~\cite{OpenAIBlog.2018} with impressive dexterity.

\subsubsection{Code As Natural Text}\label{Natural-Hypothesis}\label{Semantically-Brittle}
The so-called 'natural hypothesis' of code holds that coding is a form of communication, and large code corpora manifest patterns similar to those in natural language. The first evidence for this assumption has be brought forward by \cite{Hindle.2012}.
Source code shares some characteristics with natural language text: repetiveness of certain structures and common patterns, localness (repetitions occur in a local context), and long-term dependencies (e.g. a 'try' requiring an 'except' later in python, like the natural language statements 'on the one hand' and 'on the other hand') \cite{Dam.2016}.
Because code is usually written by humans who tend to prefer conventional, familiar and typical code \cite{Allamanis.2018}, patterns and typical structures inevitably emerge. Following this line of argumentation, machine learning models used in natural language modelling can be applied to code as well, since the core strenght of machine learning lies precisely in the ability to uncover and generalize patterns and handle noise. Furthermore, insecure code often contains several vulnerabilities of a similar flawed pattern, e.g. a missing check before a function call. Those vulnerabilities can be found easily if their similarity is recognized \cite{Yamaguchi.2012}. Finally, machine learning models can analyze large repositories without requiring the source to to be compiled, and they allow for a finetuning of precision and recall, giving them an at least promising advantage over both static and dynamic approaches \cite{Russell.2018}.
Code however is different from natural language in so far that it is 'semantically brittle' \cite{Allamanis.2018}, because small changes to code can already alter the functionality completely. It also contains highly complex structural information, e.g. in nested loops and similar constructs. Despite those differences, NLP-inspired models have been applied successfully to software code.
%Mining Fix Patterns for Findbugs Violations: Recently, a number of studies [35], [36], [37], [38], [39], [40], [41] have provided empirical evidence to support the naturalness of software [42], [43]. A recent work by Bui et al. [44] has provided preliminary results showing that some variants of Convolutional Neural Networks (CNNs) are even effective to capture code semantics so as to allow the accurate classification of code implementations across programming languages.


\subsection{Mining Software Repositories}\label{Mining-Software-Repositories}
The performance of machine learning techniques depends greatly on the quality and characteristics of the data to be classified ~\cite{Pang.2015}. Therefore, acquiring a large collection of high-quality data is the first and mandatory step of all machine learning on source code.\\
For many software systems, bugs and code changes are tracked with the help of version control and source code management systems \cite{Zhou.2017}. They also help share code between developers as well as giving access to the general public in the case of open source projects. With the growth in size, number and popularity in openly accessible projects hosted on such platforms, it has become possible to gather large amounts of code and utilize data-driven approaches to discover vulnerabilities \cite{Russell.2018}. In contrast to works focusing on just a handful of projects to train and validate their models, potentially hundreds and thousands of respoitories can be used to draw much more general conclusions. Furthermore, the repositories are generally real-life projects of actual applications, which makes them more relevant than vulnerability datasets with artificially constructed examples. Besides mining the actual code, software repositories can also be investigated to gain insights about dependencies, commit messages, patterns of changes and various metrics \citep{Liu.2018}.\\
\paragraph{Approaches for Mining Software Repositories}\mbox{}\\
According to \cite{Kagdi.2005}, approaches can be divided in mining via annotations, via heuristics, via differencing, and via data mining. The first approach focuses on commit messages and metadata and analyses e.g. which classes change how often, which classes change together, how often changes occur in a particular subsystem and so on. The second approach focuses on heuristics about developers, code layout or code layout. Mining via differencing Source-code repositories contain differences between versions of source code. Therefore, MSR can be performed by analyzing the actual source-code differences. In mining via differencing, the source code differences between two versions are analyzed, for example by determining how many functions or function calls are inserted or deleted, how many conditions are changed or how an abstract graph representation changes. Finally, with data mining, cross connections between changes and associations are found. For example, \cite{Zimmermann.2005} looked at combinations of functions that were often changed together and derived suggestions from there, preventing developers from making errors due to incomplete changes and predicting likely changes.\\
Many of those approaches take high-level information into account, e.g. classes, functions and metadata, while differencing and Data Mining can in principle be applied at a very fine level, down to single code statements.
\paragraph{Tools for Mining Sotware Repositories}\mbox{}\\
Mining software repositories is a task not suitable for manual work, therefore tools are used to assist with it. The main purposes of those tools are data extraction, data filtering, pattern finding and prediction tools. ~\cite{Chaturvedi.2013} provides an overview of pre-existing tools and datasets used in mining software repositories and find that data retrieval and preprocessing are frequently the most time-consuming task in the whole process. Their findings also suggest that many researchers decide to develop their tools and scripts themselves, while only some rely on already developed tools. 
\paragraph{Mining Github}\mbox{}\\
Github is a provider of hosting services based on the version control and source code management system Git. The scale of the available data that can be found on the platform is enormous - in summer 2018, Github reached 57 million repositories (roughly half of them public open source projects) with over 28 million users~\cite{Github.com.b}. At the time of this writing, a search for public python repositories returns close to 2 million results. This is what \cite{Allamanis.2018} et. al call 'Big Code', and those massive amount of data can be used for gaining insight into typical vulnerabilities without relying on hard-coded patterns.\\
Data from github has frequently been used in current research to train machine learning algorithms to detect bugs and vulnerabilities, see for example ~\cite{Zhou.2017}, ~\cite{Russell.2018} and ~\cite{Liu.2018}. Of course, the base rate of commits that are relevant for security is relatively low, which is why efforts to gain a large dataset of vulnerable and fixed code, the search must be tailored accordingly.\\ 


\subsection{Python Vulnerabilities}

To create a tool that finds all security vulnerabilities in a given piece of source code is a very broad task, as there are many types and variations of different vulnerabilities. In section \ref{Methodology}, the choice of vulnerabilities made here is justified in more detail. 

The types of vulnerabilities are described in the following section. Note that all examples given are extremely simple and just intended to illustrate the general idea, and exploits are often much more complex in practise.

\subsubsection{Injection Attacks}
An injection attack is based on user input that causes unintended or dangerous behavior when interpreted or executed. Exploiting an injection vulnerability usually allows the user to make the interpreter (e.g. the server or the operating system) execute arbitrary commands, and sometimes access or alter data without authorisation~\cite{Micheelsen.2016}. Injection attacks can be prevented by checking all user input and applying so-called 'sanitization' techniques that convert harmful into harmless inputs, e.g. by filtering out special characters. 
\paragraph{SQL injection}\mbox{}\\
According to the OWASP foundation, SQL injections are among the top security vulnerabilities~\cite{OWASPFoundation.}, belonging to the most common and serious vulnerabilities that are affecting web applications.\\ 
The Common Weakness Enumeration defines a SQL injection as following: \textit{The software constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component}~\cite{CommonWeaknessEnumeration.19.9.2019}. When user-controllable input contains SQL syntax that is not removed, it can be interpreted as SQL statement instead of user data and executed. This can be exploited to alter querys, e.g. to access files that should not be accessible or to add additional statements changing or destroying databases. Any kind of database-driven website is potentially in danger of becoming the target of such an exploit if their sanitisation is not thourough.\\
The following snippet illustrates such a vulnerability.
\begin{lstlisting}[language=Python,showstringspaces=false]
(...)
cur = db.cursor()
name = raw_input('Enter name: ')
cur.execute("SELECT * FROM users WHERE username = '" + name + "';")
(...)
\end{lstlisting}
If the user enters a legitimate request, e.g. \texttt{Tom}, then all goes well and the query has no negative effects. However, if the user inserts a string containing SQL code, like \texttt{Tom'; DROP TABLE users;}, the semicolon is interpreted as the end of the query and everything afterwards is executed as a new command, deleting the entire database table. If the code is changed like this:
\begin{lstlisting}[language=Python,showstringspaces=false]
(...)
cur = db.cursor()
name = raw_input('Enter name: ')
cur.execute("SELECT * FROM users WHERE username = %s;", (name,))
(...)
\end{lstlisting}
The paramete passed after the comma is escaped, not directly substituted, in the place of the placeholder \%s. Any SQL code tokens are removed, and the request can be executed safely. This is of course only one option to fix the problem, but it illustrates the general need for filtering and sanitizing the user-provided input. 

\paragraph{Command injection}\mbox{}\\
Quoting again the Common Weakness Enumeration: \textit{The software constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component.}~\cite{CommonWeaknessEnumeration.23.9.2019} This is another case where untrusted data is executed, but this time, it is not targeted at an SQL database, but instead a command that is executed by the machine that is attacked, for example the server shell. This can give an attacker the capability to read, change or destroy files they should not be able to access. \\
The following minimal example for in python 3 demonstrates the vulnerability:
\begin{lstlisting}[language=Python,showstringspaces=false]
import subprocess
filename = input('Please provide the path for the file: ')
command = 'cat {path}'.format(path=filename)
subprocess.call(command, shell=True)
\end{lstlisting}
If a user enters \texttt{file.txt}, the file is displayed with the cat command, but by adding a semicolon, the user can append any other commands which are then executed as well without question. For example, entering \texttt{file.txt; ls} would cause the shell to print the current directory, and \texttt{file.txt; rm -rf /} would cause a considerable amount of damage. This vulnerability could have been prevented by a) not calling subprocess with shell=True, and b) sanitising or filtering the input. 


\paragraph{Remote Code Execution}\mbox{}\\
The term 'Remote Code Execution' describes a similar situation to command injection, the difference being mainly that actual programming code is executed on the target system, while in command injection, it's an OS system command that is executed. Sometimes, it is also used to describe a hacking goal rather than a vulnerability, in the sense that exploting a vulnerability grants the attacker the capability to execute arbitrary commands on a system. 


\subsubsection{Various types of session hijacking}
The core idea of \textbf{session hijacking} is to steal the connection between the server and the client, stealing or guessing a valid session token to pretend to be an authorized client.~\cite{OWASPFoundation.14.8.2014}\\
\textbf{Session fixation} works by tricking a user to start a connection with a client using a session ID that is set by a malicious third party, for example by making the user click a link with the session ID as a parameter. Since the malicious third party then knows the session token, the active session can be accessed. The attacker might even be able to access a logged in account.
~\cite{OWASPFoundation.14.8.2014}\\
\textbf{Cross Site Scripting} (see below) can also be used to steal a session token, enabling the attacker to hijack the client's and server's shared session.\\
Another type of session hijacking are \textbf{man-in-the-middle attacks}. An attacker tries to intercept a communication between two systems, e.g. a client and a server, posing towards both sides as their connection partner. The attacker effectively acts like a proxy and can therefore access the content of the communication and sometimes even modify it. ~\cite{OWASPFoundation.31.8.2015} The use of proper encryption and certificates prevents man-in-the-middle attacks.\\
The term \textbf{replay attack} describes an attack in which a valid part of communication between two partys (e.g. client and server) is recorded and later sent again by the attacker, who is posing as the original sender of the transmission. If no proper protection mechanisms are in place (mainly secret one-time session IDs), the attacker can gain access to functionality and information that was only intended to be accessible for the original sender.


\subsubsection{Cross Site Scripting}
Cross site scripting, often abbreviated as XSS, is one of the most important vulnerabilities in web applications. It appears frequently in the OWASP top ten vulnerabilities~\cite{OWASPFoundation.}.\\
In cross site scripting, unsanitized data is also the root of the problem. This time, custom code inserted by a user is added to a website or an url that is then delivered to other users, who will receive the code as part of the website and execute it in their browser. The CWE defines Cross Site Scripting as following: \textit{The software does not neutralize or incorrectly neutralizes user-controllable input before it is placed in output that is used as a web page that is served to other users.}~\cite{CommonWeaknessEnumeration.19.09.2019}\\
A classic example (in this case stored cross site scripting) is a guest book in which arbitrary input is allowed. A guest can leave plain text, or a piece of javascript that is then permanently stored on the site and delivered to other visitors, who will receive and execute the javascript code. Of course, many variations of this are possible, e.g. using different kind of input options and creating executable code with Flash and other languages. Another example is an email with a link to anothers website that contains malicious javascript within the url, which will then be executed upon following the link. 
To prevent XSS attacks, user generated content has to be sanitized, e.g. using the \texttt{html\_escape} function.\\
An example taken from \cite{Micheelsen.2016} shows a vulnerable snippet of python code using the web framework flask. The input 'param' is taken from the user and inserted in the result page via the html.replace function.
\begin{lstlisting}[language=Python, showstringspaces=False]
@app.route('/ XSS_param',methods =['GET'])
def XSS1():
  param = request.args.get('param','not set')
  html = open('templates / XSS_param.html').read()
  resp = make_response(html.replace('{{param}}',param ))
  return resp
\end{lstlisting}


\subsubsection{Cross Site Request Forgery}
The CWE defines Cross Site Request Forgery as \textit{The web application does not, or can not, sufficiently verify whether a well-formed, valid, consistent request was intentionally provided by the user who submitted the request} ~\cite{CommonWeaknessEnumeration.19.9.2019b} and further explains: \textit{When a web server is designed to receive a request from a client without any mechanism for verifying that it was intentionally sent, then it might be possible for an attacker to trick a client into making an unintentional request to the web server which will be treated as an authentic request. This (...) can result in exposure of data or unintended code execution.} To illustrate this with an example: A badly protected website might receive a POST request from a user which contains parameters to change the password. This request could have been generated by the user intentionally clicking on a html form that generates the request - or because a malicious person tricked the user into clicking a link with the same parameters that triggers the POST request. The password is changed either way, and the user might be locked out of their account while the attacker has access. This kind of attack can be prevented by using tokens that are exchanged between client and server that is secret and unique for each request. The following example shows how a token could be used to prevent XSRF attacks.
\begin{lstlisting}[language=Python, showstringspaces=False]
from oauth2client import xsrfutil
(...)
def CheckToken(self, *args, **kwargs):
  user = users.get_current_user()
  token = str(self.request.get('xsrf_token'))
  if not user or not xsrfutil.validate_token(_GetSecretKey(), token, user.user_id()):
    self.abort(403)
  
\end{lstlisting}

\subsubsection{Directory Traversal}
A path traversal or directory traversal vulnerabiliy occurs when the user is able to manipulate input in a way that paths of the file system are exposed that were not meant to be accessed. 
As defined by the CWE: \textit{The software uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the software does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory.}~\cite{CommonWeaknessEnumeration.19.9.2019c}  A typical case of this vulnerability is a website that displays a file for which the path is specified in a url paramter. By changing this parameter to contain some '../../..', the attacker can navigate the file system and possibly display files that were not meant to be accessed.\\
Assuming that \texttt{filepath} is a variable provided by the user, the following example shows a vulnerable code segment.
\begin{lstlisting}[language=Python, showstringspaces=False]
import os.path
(...)
prefix = '/home/user/files/'
full_path = os.path.join(prefix, filepath)
read(full_path, 'rb')
\end{lstlisting}
A possible way to fix this would be to use the \texttt{os.path.abspath} method on the user provided path and check whether if the prefixes are still the same. 



\subsubsection{Open Redirect Vulnerabilities}
As the name suggests, open redirect vulnerabilities occur when a website that is supposed to redirect a user gets the target of the redirect from a parameter value. When this url paramter is manipulated, the redirect sends the user to the wrong page, e.g. a malicious page crafted by an attacker. The python web framework django contains a check called \texttt{is\_safe\_url} (later renamed to \texttt{url\_has\_allowed\_host\_and\_scheme}) which can be used to perform a check to prevent open redirect vulnerabilities as shown in the following examples.

\begin{lstlisting}[language=Python, showstringspaces=False]
next_url = request.GET["next"]
return next_url
\end{lstlisting}

\begin{lstlisting}[language=Python, showstringspaces=False]
from django.utils.http import is_safe_url
(...)
if is_safe_url(request.GET["next"]):
  next_url = request.GET["next"]
return next_url
\end{lstlisting}




%man in the middle
%tampering
%remote code execution 


\section{Related Work}\label{Related-Work}
Those examples all relate to security in some way, however, since this work deals with finding vulnerabilities in code that is supposed to be secure, the following sections will be limited to that specific area of research.\\

\subsection{Vulnerability Prediction based on Software Metrics}\mbox{}\\

What are the features to use when predicting whether code is vulnerable or not? For a long time, the most commonly used features were software and developer metrics, which includes include size of the code, cyclomatic complexity, code churn, developer activity, coupling, number of dependencies or legacy metrics~\cite{Morrison.2015}. They have been universally used as features for building fault prediction models~\cite{Hall.2011} and are highly relevant in the field of software quality and reliability assurance, making them an obvious tool to apply to vulnerability prediction. To give just two examples, ~\cite{ACM.2008b} use change metrics and ~\cite{Nagappan.2008} use organiational metrics to predict faults in software. \\
By their nature, software metrics do not capture the semantics of the code ~\cite{Shin.2008}. In many cases, it is possible that two pieces of code have the same metrics (e.g. complexity) but a totally different behavior, leading to a different likelihood to be vulnerable. In addition to that, the predictors are manually chosen by knowledgable domain experts and therefore inherently subjective, and thus may carry biases or oversights\cite{Dam.2017}. They also tend tonot generalize well from one software project to the next. The strongest criticism is that this approach does not take the actual source code, the program behavior or the data flow into account, effectively applying a foregone conclusion that certain features will be related to security flaws that is not necessarily true \citep{Hovsepyan.2012}. For example, many vulnerabilities can also arise in very simple programs, and often, the trivial or direct solution to an algorithmic problem does not contain the safeguards and precautions that are required for preventing exploits. Therefore, code complexity is not an ideal predictor for security flaws, and similar arguments and counter examples can be found for the other metrics as well. However, it is undeniable that at least \textit{some} insights can be gained from software metrics, as the following works illustrate. Most of them propose supervised machine learning techniques that utilize code metrics to predict the occurence of security-related flaws in software.\\

~\cite{Shin.2008} use nine complexity metrics to predict vulnerabilities in Javascript projects, achieving a low false positive rate, but a relatively high false negative rate.\\
In a later work, ~\cite{Shin.2010} leveraged code complexity, code churn and developer metrics to predict vulnerabilities, achieving 80\% recall and 25\% false positives with linear discriminant analysis and bayesian networks. 

Using complexity, coupling and cohesion metrics (commonly abbreviated as CCC), ~\cite{Chowdhury.2011} try to predict software vulnerabilities using approaches that have been applied to fault detection before. They conduct a study on releases of Mozilla Firefox and use decision trees, random forest, logistic regression, and naive bayes models to predict vulnerabilities, achieving around 70\% precision and recall, respectively. 

Zimmerman et al. added even more metrics to the list: they investigated code churn, code complexity, dependencies, code coverage, organizational measures and actual dependencies ~\cite{Zimmermann.2010}. They found a weak, but statistically significant correlation between the investigated metrics and used logistic regression to predict vulnerabilities based on them, focusing on the proprietary code of Windows Vista. The metrics were able to predict vulnerabilities with an average precision (median precision was 0.60), but with a relatively disappointing recall (median recall was 0.40). 

Neuhaus et al. ~\cite{Neuhaus.2007} looked at import statements in the Mozilla project, reporting an average precision of 70\% and recall of 40\% when predicting vulnerabilities by import statements. 

Other researchers have been able to use make predictions just with commit messages. \cite{Zhou.2017} leverage a K-fold stacking algorithm to analyze commit messages to predict whether a commit contains vulnerabilities, reportedly with great success. In contrast, \cite{Russell.2018} found that both humans and Machine Learning algorithms performed poorly at predicting build failures or bugs just from commit messages.

Those approaches require knowledgeable domain experts to determine the metrics that are used as predictors for vulnerability. As pointed out above in section \ref{Motivation}, it is desireable to reduce the amount of human work required for vulnerability prediction, and find alternatives to features chosen by subjective experts. The following sections explore possible ways to achieve those goals.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\linewidth]{img/Overview}
	\caption{One way to structure different approaches for vulnerability detection}
	\label{fig:overview}
\end{figure}

\subsection{Anomaly Detection Approaches for Finding Vulnerabilities}\mbox{}\\
Anomaly Detection refers to the problem of describing normal and expected behavior and detecting deviations from it, since not-conforming to the implied rules can often be the cause of a defect. Data mining techniques have been used to analyze source code and extract normal coding patterns.\\
To name one example, \cite{Li.2005} delevop a tool called PR-Miner that can find code patterns in any programming language and that has been proven to be quite useful. Their approach, which relies mostly on associating programming patterns that are used together with each other, is independent of any chosen language and violations reported by their tool have been confirmed as bugs in Linux, PostgreSQL and Apache. A fundamental problem is, however, that bugs that are \textit{themselves} are typical patterns (and therefore occur frequently in the code) are systematically overlooked, resulting in common flaws not being detected \cite{Yamaguchi.2012}. At the same time, rare programming patterns or API usages can be flagged as false positive simply because they don't occur often.\\
Several of the anomaly detection approaches have quite high false-positive rates \citep{Ghaffarian.2017}. Specifically for finding security vulnerabilities (and not mere bugs that don't have any implication for security), anomaly detection in code is not a straightforward approach, since it is hard to tell when a violation of common code patterns has an implication for security and when it doesn't.\\
%This approach in this work differs from anomaly detection insofar as explicit labels are used to train a model on vulnerable and (mostly) secure code, thereby avoiding the questionable assumption that "typical" equals "correct".  

\subsection{Vulnerable Code Pattern Analysis and Similarity Analysis}\mbox{}\\
In comparison to learning what meta properties of code are found in combination with vulnerabilities, or of learning what correct software typically looks like and finding violations of that, it seems almost like the most natural choice to try to learn what vulnerable code looks like to find vulnerabilities. This is also what is attempted in this work.
There are two slightly different strategies: one is to learn typical vulnerability features or patterns and to recognize them later in unknown code. The other is similarity analysis, in which the source code is first divided in fragments and represented in an abstract fashion (including graphs and abstract syntax trees) before those are compared, assuming that the representations of vulnerable code fragments share some properties with each other. They work best for identical or nearly identical code clones in which the inherent structure of the compared code fragments is very similar\cite{Li.2018}. The following section, however, focuses on code pattern analysis, since this is the approach taken in this work.\\
In vulnerable code pattern analysis, vulnerable code segments are analyed with data-mining and machine-learning techniques to extract their typical features and patterns that can then be applied to new code segments to find vulnerabilities. Most of the works in this area gather a large dataset, process it to extract feature vectors, and then use machine-learning algorithms on it, as described by \citep{Ghaffarian.2017}. The main advantage is that the identifying features can be created automatically or semi-automatically, eliminating the need for subjective human experts. By learning directly from a dataset of code what vulnerable code entails, an unbiased model can be built.\\
In many cases, those approaches also rely on a very rough granularity, e.g. classifying whole programs ~\cite{Grieco.2016}, files ~\cite{Shin.2010}, components ~\cite{Neuhaus.2007} or functions ~\cite{Yamaguchi.2011}, which makes it impossible to pin down the exact location of a vulnerability. Some, like \cite{Li.2018} and \cite{Russell.2018}, use a more fine-grained representation of the code. Furthermore, the approaches differ in the scale of their dataset, some comparing just a few files from one project, others looking at millions of functions. Even more interestingly, there is a wide variety of feature selection and classification methods used. Some of the most relevant works are described in the following section in more detail.\\

~\cite{Morrison.2015} examine security vulnerabilities in Windows 7 and Windows 8 with various machine learning techniques including logistic regression, naive bayes, support vector machines and random forest classifiers, with relatively disappointing results, achieving very low precision and recall values.\\
In a relatively straightforward approach, ~\cite{Pang.2015} take labels from an online database and use a mix of feature selection and n-gram analysis to classify whole java classes as vulnerable or not vulnerable. Working on a relatively small dataset of four Java android applications, they apply a simple n-gram model, in which a vector with a component for each language token stores the number of occurences of those tokens. The n-gram approach is simple, but does not take the order of the tokens into account, which might be a problem in programming languages, which are semantically brittle (See section \ref{Semantically-Brittle}). Also, n-gram vectors often require huge dimensionality depending on the size of the program, which can be a practical problem on its own. Therefore, Pang et. al use feature selection (or better: ranking) methods to combine related features and reduce the number of irrelevant features taken into account. Afterwards, they pick Support Vector Machines as learning algorithm, achieving around 92\% accuracy, 96\% precision and 87\% recall within the \textit{same} project, and values around 65\% in cross-project prediction (training on one project and trying to classify vulnerable files in another one). \\
~\cite{Shar.2013b} apply machine learning to reduce false positives in spotting XSS and SQLI vulnerabilities in PHP code. They first pick some code attributes manually and then train a multi-layer perceptron to complement static analysis tools. Compared to a static analysis tool, they detected less vulnerabilities, but also achieved lower false positive rates in an overall satisfying result. In their later work ~\cite{Shar.2013}, they use a hybrid approach including dynamic analysis, improving their previous results notably, as tested on six big php projects. They also experiment with unsupervised predictors, which are less accurate, but still a promising area of research. \\
~\cite{Yamaguchi.2012} use abstract syntax trees to encode and describe small portions of code as a composition of basic patterns. They then embed those in a vector space and perform latent semantic analysis, a technique of natural language processing, to identify topics. Using those, they are able to accurately find very similar functions to a given function that contains a vulnerability, so it is easier to spot similar flaws that are spread across the entire code base. Their work is based on four open source repositories.\\
~\cite{Hovsepyan.2012} analyse raw source code as text. As an example, they picked an Android email client written in Java and mostly focus on analyzing the source code like a natural language, analyzing files as a whole. After filtering out comments, they transform files in feature vectors made up from java tokens with their respective counts in the file (in a bag-of-words-style approach). Those feature vectors are classified in a binary scheme as vulnerable or clean. Finally, the classifier (a Support Vector Machine) is trained to predict if a file is vulnerable. The accuracy achieved by this classifier is 0.87, with 0.85 precision and 0.88 recall. Their success shows that much insight can be gained without elaborate models of code representation by just taking the source code as natural text and analyzing it as-is. Their work is limited by the application on a single software repository. In a later work, [] used decision trees, k-nearest-neighbour, naive bayes, random forest and support vector machines for a similar task ~\cite{Scandariato.2014}.\\
\label{Bag-of-Words}The bag-of-words-approach eliminates the need for manually designing features, but it also has some drawbacks. It cannot recognize semantic relationships between code tokens, e.g. 'if' and 'else' or 'for' and 'while', and it doesn't capture the semantic structure of code which is inherently sequential \cite{Dam.2017}. This syntactic information is completely disregarded. The recent advances in more sophisticated deep learning models, especially recurrent and convolutional neural networks, make it possible to train models that deal better with this challenge.\\
\paragraph{Deep Learning for Vulnerability Prediction}\mbox{}\\
Deep learning has recently attracted increasing interests in software engineering. A number of approaches have already successfully leverage deep learning models to automatically learn feature for fault prediction ~\cite{IEEE.2015b,IEEE.2016,Wang.2016}. More about the applicability of those networks to tasks related to software code has been said in sections \ref{Deep-Learning} and \ref{RNN}. But how can they applied to specifically spot vulnerabilities in code? Several researchers have targeted this question.
Recurrent neural networks and convolutional neural networks are used by \cite{Russell.2018}, who scrape a big codebase of C projects from github, the Debian Linux distribution, and 'synthetic' examples from the SATE IV Juliet test suite, collecting a database of over 12 million functions in total. They use three different static tools to generate the binary labels 'vulnerable' and 'not vulnerable' for the functions and a randomly initialized one-hot embedding for lexing. For the core part of their work, Convolutional Neural Networks and Recurrent Neural Networks are explored for feature extraction, followed by a Random Forest classifier (as the neural networks did not perform well on classification on their own). The convolutional neural networks performed best, allowing for finetuning of precision and recall against each other. With their work, Russel et al are not only among the first researcher to use deep representation learning directly on source code from a \textit{large} codebase, but they are also able to use a convolutional feature activation map to highlight the suspicious parts in the code, instead of just classifying a whole function as vulnerable.\\
~\cite{Liu.2018} base their work on the assumption that violations that are routinely fixed are actually true positives, while those that are ignored are likely to be either not important, or false positives. They investigate changes from 730 Java projects, apply the static bug detection tool Findbugs to find changes that are fixing a violation reported by that tool, and track the violations throughout the different versions to find whether they are fixed or ignored. Using this data, they can analyze which violations reported by the tool are routinely ignored over many revisions, and which others are fixed almost immediately. They collect the code patterns corresponding to violations based on representation with an abstract syntax tree. Instead of training a binary classifier on 'vulnerable' or 'not vulnerable', Liu et. al use an unsupervised learning approach to extract features of code, focusing specifically on patches to learn fix patterns. The code patterns are encoded into a vector space using Word2Vec, the discriminating features are learned with a Convolutional Neural Network, and a X-means clustering algorithm is used to cluster violations with learned features. They find that while security related violations are relatively rare (0.5\% of violation occurences), they are widespread across 30\% of the projects. Also, the works shows that only a small fraction of violations is fixed. Looking into the code patterns of fixed violations, Liu et. al find that for 90\% of the violations, a chunk of just 10 lines of code or less is sufficient to capture the relevant context. The CNN yields patterns that are largely consistent with the tool's violation description. The approach results in the generation of fix patterns. Roughly one third of a test set of violations can be fixed with one of the top five presented fix patterns. Liu et. al also chose 10 open source Java projects to make suggestions to based on fixes proposed from their tool, and of the 116 suggestions, 67 have been immediately merged. Of course, their tool can only suggest patches that correspond to fix patterns previously found in the database. \\
\paragraph{Long Short Term Memory Networks}\mbox{}\\
Although ~\cite{Gupta.2017b} and ~\cite{Dam.2016b} have already shown that Long Short Term Memorys are highly suitable for modelling source code and fixing errors in C Code, \cite{Dam.2017} were probably the first to leverage LSTMs networks to automatically learn features for predicting \textit{security vulnerabilities}. They take a publicly available dataset consisting of 18 java applications and extract the code of all methods within the source file, using Java Abstract Syntax Tree and replacing some tokens with generic versions. They then use LSTMs for training syntactic and semantic features and a Random Forest classifier. They achieved around 91\% precision for within-project prediction of vulnerabilities, and on average, after training a model on one project, it achieved more than 80\% precision and recall in 4 of the other 17 projects.\\
The tool VulDeePecker is a deep-learning based vulnerability detection system \cite{Li.2018}. The authors present the first dataset of vulnerabilities intended for deep learning approaches, which is not a database of natural code, but stems from popular C and C++ open source producs, derived from the National Vulnerability Database and the Software Assurance Reference Dataset maintained by the NIST. Li et. al strive to create a tool that does not rely on humans to define features and still provised a satisfyingly low rate of both false negatives and false positives. They split files into so-called code-gagdets, semantically related lines of code that are grouped together, focusing on key points of library and function API calls in a relatively complex mechanism. They only evaluate two different kinds of vulnerability: buffer errors and resource management errors (which both have lots of subtypes). Li et al decided on bidirectional Long Short Term Memory networks on different subsets of their data, achieving a precision of around 87\%, with improved results if the network is trained on manually selected function calls. They also managed to detect four previously unknown vulnerabilities in software projects. \\
~\cite{Harer.2018} trained LSTM networks to detect and fix vulnerabilities in the synthetic SATE IV code base of C vulnerabilities. They were able to leverage a sequence-to-sequence approach to produce fixes for found vulnerabilities, although it is hard to measure and compare their success. Similarly, ~\cite{Gupta.2017} use RNNs in a sequence-to-sequence setup to fix buggy C code, although they are not focusing on security vulnerabilities, fixing 27\% of their programs and 19\% partially.


\section{Open challenges addressed by the thesis project.}

This project expands the work in the area of vulnerable code pattern analysis. The main goal is to collect a large body of github projects written in Python and train a neural network to correctly identify vulnerabilities. More specifically, the code is to be used as text, not abstracted in any way. LSTMs are leverage to learn features and classify code segments as vulnerable or not vulnerable. 

\begin{itemize} 
	\item How can a large and usable labeled data set of python code be acquired efficiently from github? Specifically, how can data be gathered that is useful for vulnerability research?
	\item Can a Word2Vec model be trained on python source code and function as an embedding layer for a neural network?
	\item Are labels taken from the commit context good enough for actually training a neural network on a meaningful distinction between vulnerable and non vulnerable code?
	\item Is a purely textual representation of source code enough? %AST
	\item How does the proposed tool compare to other research efforts? How well does it perform in various metrics (precision, recall, accuracy and F1 score)?
	\item Can vulnerabilities be found that are subsequently actually accepted as corrections to open-source software projects?
\end{itemize}
%TODO more research questions

In contrast to \cite{Li.2018} and \cite{Dam.2017}, this approach uses a wide code base and not only a select number of projects. In further contrast to \cite{Dam.2017}, vulnerabilities are not just detected on the file level, but in specific positions within the code. 
Furthermore, this approach will use changes in github commits as a basis for determining labels in code, as opposed to relying on already existing static analysis tools, as e.g.  \cite{Russell.2018} do. This enables, in principle, the discovery of vulnerabilities patterns that have not yet been manually included in static analysis tools. By using the whole experience of contributors uploading and modifying their code on github, the approach is agnostic towards specific projects with specific characteristics and to an extend robust against the threats to validity that come with a more narrow approach. To conclude the list of contributions, the focus lies on code written in Python, in contrast to most other research projects that are mostly concerned with Java, C, C++ or PHP. The proposed approach however could be applied to other languages as well. \\
This works picks a less simplified approach for feature selection than ~\cite{Pang.2015}, that more relevant information that is relevant for code semantics (e.g. order of tokens) will help with the classification, and use neural network techniques instead of support vector machines for classification. \\
Similarly to \cite{Hovsepyan.2012}, this work does not transform source code into a structure like an abstract syntax tree, but takes it as plain text. However, the scope is much bigger: instead of a single project, thousands are incorporated into the corpus used for training the classifier. \\
This approach is similar to the one taken by \cite{Russell.2018}, but is aimed at python code and uses a different type of neural network. \\
Like \cite{Li.2018}, this work uses LSTMs to learn and classify vulnerable code tokens, but in contrast, it does so on a large database of natural code and aims to detect many different vulnerabilities. \\
In comparison to \cite{Dam.2017}, we use a simpler architecture of our machine learning model and a different approach for embedding and feature selection, but also a much larger dataset. \\
The basic assumption from ~\cite{Liu.2018}, namely that \textit{fixed} vulnerabilities are basically proven to be not false positives, is also a foundation of this work. But in contrast to the work of Liu et. al, the fact that a vulnerability was fixed in a commit on Github is the only evidence that is taken into account, and no analysis tool is used in the first place to identify vulnerabilities.

%RQ1: Can VulDeePecker deal with multiple types of vulnerabilities at the same time? A vulnerability detection system should be able to detect multiple types of vulnerabilities at the same time, because multiple detection systems need to be maintained otherwise. For answering this question, we will conduct experiments involving one or multiple types of vulnerabilities. [Liu 2018]


\section{Methodology}\label{Methodology}
In this section, some preliminary guiding principles for the approach taken in this work will be discussed, including the scope, sources of data, choices of data representation and selection of neural networks for the task. 
%We discuss some preliminary guiding principles for this purpose, including the representation of software programs to make deep learning suitable for vulnerability detection, the determination of granularity at which deep learning-based vulnerability detection should be conducted, and the selection of specific neural networks for vulnerability detection. [Liu2018]

\subsection{Choosing a programming language}
Most of the previous works have either trained their models on very small corpora, as pointed out by ~\cite{Bhoopchand.2016}, or have been focusing on statically-typed languages like Java, C and C++ \cite{Bellon.2007,Russell.2018,Liu.2018,Dam.2017, Rolim.2018}, and several groups of researchers were kind enough to present their databases as publicly available training set. It seems therefore more interesting to focus on a programming language that has received not as much attention by researchers. According to several online rankings, python is one of the most important and popular programming languages ~\cite{AyeshaCuthbert.15.4.2019, VidushiDwivedi.}, and it's the third most used language on Github ~\cite{Github.com.19}, after Javascript and Java.\\
There is so far no Word2Vec model trained on python code available, so this is also a possible contribution of this work.\\

\subsection{Choosing Vulnerabilites}


\subsection{Data Source}\label{diff}
As described in the section about related works, other researchers have used a wide variety of datasets to train models for classifying vulnerabilities, some only working with a hand full of projects, others using thousands or even millions of functions. This work boldly aims to make use of a large dataset that deserves the name. Furthermore the full dataset is gathered from projects publicly available on Github, for several reasons: First, the data is public, making it easier to re-examine and replicate the work. Second, in contrast to synthetic code bases, nearly all projects on github contain 'natural' source code in the sense that they are actual projects used in practise. And third, Github is the largest host of source code in the world, so an ample amount of data is available.\\
Since Github is mainly also a version control system, it is centered around commits, and as \cite{Zhou.2017} suggests, it is practicable to look at commits to detect vulnerabilities. Commits that fix a bug or vulnerabilities can be described as patch, consisting of a pair of versions, one buggy and one updated and (hopefully) correct. In the GNU diff representation and similar representations also used by e.g. Github, a '+' at the start of a line indicates the new and fixed line, while a '-' indicates the line was removed in favour of the fix.~\cite{Liu.2018} On github, a commit can include changes to several different files at once. By analyzing the differences between the old and the new version, fix patterns can be learned that describe how the code was improved. \\
For the purpose of this work, the goal is to find a large amount of python commits that fix a problem related to security. Due to the restrictions of the github search API, only certain kinds of requests can be made, and for every request, the number of results is limited to 1000~\cite{Github.com.2}. In contrast to the regular search available for users~\cite{Github.com.2019}, filters can not be applied in the search API, so it is not possible to filter e.g. for just the programming language python.\\
Therefore, the approach chosen here is to write a script that makes use of the Github API to search for commits with various security-related search terms, and then filter out everything that is not relevant, e.g. code in a different programming language, or config files. Similarly to ~\cite{Hovsepyan.2012} and many other works, comments are also filtered out from the files itselves, as they are unlikely to influence the vulnerability of a file.\\
After finding the commits, the github diff files are downloaded, which can be achieved in a single simple http request, as they are provided in plain text. This is a much easier approach than to clone the full repository and selective pick the whole files from a certain point in the history of a project, which is simply not feasible due to the size of our dataset and computational and time constraints. Instead of collecting full files related to the commits, this work is restricted to the comparatively small diffs.
\begin{figure}[ht]
	\centering
	\includegraphics[width=\linewidth]{img/GitCommitPreviousAfter}
	\caption{Retrieving the snippet in the state before and after the commit from a git diff, old vulnerable version in red, new version in green}
	\label{fig:gitdiff}
\end{figure}\\
The result of the previous step is big collection of code diffs which can be used to reconstruct the relevant lines of code in the state before and after the patch. For every changed file, the diff from Github provides the changed lines as well as three lines before the first change and three lines after the last, so there is not much context for the change. %TODO 450 tokens
But it is very well possible that the large number of changes that can be mined with this strategy make up for the relatively narrow context provided  for each change.

\subsection{Labeling}

Similarly to \cite{Li.2018}, the data is labelled from the commit context. Since the data consists of security related fixes, the version of code snippet before the change is labeled as vulnerable, and the version after the fix is labeled as possibly not vulnerable. Of course, there are cases in which a fix doesn't actually solve a problem, or there are several vulnerabilities at the same time, or even a new vulnerability is introduced. All those are disregarded by this approach, as the main focus here is easy automation without the need for human expert oversight. (In contrast to \cite{Li.2018}, this work does not include a manual check afterwards, as it would be totally impractical for the size of the dataset). Furthermore, it needs to be stressed that everything labeled as 'not-vulnerable' is supposed to be interpreted as 'at least not proven to be vulnerable'. 

\subsection{Representing of Source Code}


\subsubsection{The case against syntax trees}
There are different approaches of how to deal with source code. Some researchers have gone to great lengths to use create abstract syntax trees from their code and operate on them ~\cite{Ma.2017,Yamaguchi.2012}. Abstract syntax trees provide a hierarchical representation of either the code or the changes applied to the code~\cite{Liu.2018}, and can be used in different styles, ommiting more or less tokens to simplify the tree. ~\cite{Yamaguchi.2012} lays out several different approaches: only API nodes and function names can be used as nodes in the tree, cutting off everything else; subtress of depth n with API nodes and placeholders can be used; or subtress including api nodes, placeholders and syntax nodes. In most cases, however, many parts of the code are not represented in the AST.\\
Some researchers state that it is too challenging to mine patterns from plain text code ~\cite{Liu.2018} and the AST notation is therefore neccessary. Note that there are counterexamples for that showing that working on the plain text can also be a very effective approach. ~\cite{Russell.2018,Hovsepyan.2012}.\\
It should als be taken into account that Long Short Term Memory networks are designed precisely for such a task of dealing with sequential input data similar to natural text or code and have performed very well on this kind of data. Furthermore, ~\cite{Dam.2016} argues that alongside with human engineered features and software metrics, ASTs are not able to capture the semantics hidden deeply in source code. \\
While ASTs are undoubtedly helpful for many applications, for example keeping track of the very same code segment across revisions as done in ~\cite{Liu.2018}, in this work, the machine learning algorithms operate directly on the code to perform feature extraction. 

\subsubsection{Choosing Granularity}
As  ~\cite{Morrison.2015} describes, binary-level predictions and analysis on the level of whole files provide little insight, as developers often already know which files might be sensitive to security vulnerabilities, and developers strongly prefer a much finer approach, if possible at the level of lines or instructions.\\
There are many works that look at vulnerabilities on the program, file or function level, but this work uses an approach of much finer granularity, looking at each token in the code. This makes it theoretically possible to pin down the location of the vulnerability, which would otherwise not be possible. \cite{Dam.2017} provide some convincing examples at the beginning of their work, arguing that a file with similar metrics, similar structure and even nearly the same tokens can include a vulnerability or not. As was already stated before, software code has the property of being semantically brittle, so a small change (e.g. switching the order of two operations) can have large impacts. Therefore, the whole code down to the single tokens is taken into account in this work.
%[Li 2018] is also fine grained

\subsubsection{Preprocessing the Code}
In languages such as python, source-code level tokens consist of identifiers, keywords, seperators, operators, literals and comments. While some researchers exclude separators and operators ~\cite{Pang.2015}, others strip out a lot of tokens and keep only e.g. API nodes or function calls ~\cite{Yamaguchi.2012}. This work strips out comments and keeps the source code otherwise exactly as it is, similar to  ~\cite{Hovsepyan.2012}. No variables or literals are replaced by generic names, but everything is taken  exactly the way it is represented in the code. 

\subsubsection{Embedding Code in a Vector}
\begin{figure}[ht]
	\centering
	\includegraphics[width=\linewidth]{img/Word2Vec}
	\caption{transforming code in vectors}
	\label{fig:word2vec}
\end{figure}

Neural Networks work on numerical vectors with an uniform size, and therefore it is neccessary to represent code as vectors that retain the semantic and syntactic information that was present in the code. The variables of the vector have to chosen in such a way that the vectors are manageable in size.\\
While Li et al.\cite{Li.2018} use carefully crafted code gadgets, ~\cite{Hovsepyan.2012} employ a simple bag-of-words approach, ~\cite{Russell.2018} train a randomly initialized one-hot-embedding, and ~\cite{Liu.2018} leverages Word2Vec. As already described in section \ref{Bag-of-Words}, the bag-of-words-approach is not appropriate to capture the sequential structure of code. This work uses a Word2Vec embedding to transform code tokens into numerical vectors.\\
Word2Vec is a two-layer neural network designed for embedding words in a numeric vector space in a way that preserves semantic features of the text used as a basis. The cosine similarity of the vectors measures the semantic similarity, as determined by the network. If two words have nothing in common and no relation to each other whatsoever, their vectors would have a 90-degree angle, and complete similarity or equivalence is expressed as a 0-degree-angle. For code tokens, it would be expected that e.g. 'True' and 'False' have a higher similarity than 'True' and 'if', or 'True' and 'include', for that matter.\\
A code snippet is transformed in a list of representations of the single tokens making up that snippet, including language keywords, identifiers like function names and variables, numbers, operators and even whitespaces, brackets and indentations. Each of them has to be embedded, in other words, represented by a numeric vector. A snippet of code is therefore transformed into a vector of vectors of numbers.\\
Word2Vec has been successfully used for similar projects before \cite{Liu.2018} and is a more sophisticated approach than a simple bag-of-words embedding. Since there is currently no pre-trained language model for python code available, such a model first has to be trained. For this purpose, a large independent set of high-quality python code is mined from github, this time not searching for vulnerability-fixing commits, but instead just collecting the current versions of large and popular repositories. On this corpus, the Word2Vec model is trained to prepare it for the task to encode python code tokens as vectors.



\subsection{Selecting Neural Networks}

Neural networks have been successfully applied in areas like speech recognition, natural language processing and image processing, but it has to be decided carefully which ones are best suited for vulnerability detection. \\
Whether or not a line of code contains a vulnerability does not only depend on the immediate token, but also on the context, meaning the lines before and after that specific position. Dependent code elements can be spread out over several lines of code, for example in pairs like 'try' and 'except' or 'lock' and 'unlock', or when a variable is changed and later referenced. Therefore, following the argumentation of \cite{Liu.2018} and ~\cite{Dam.2017}, neural networks with the ability to store contextual information are to be prefered. Several types of Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs) and, more specifically, Long Short Term Memory Networks (LSTM) have been used in (very) recent works of research and proven to be applicable to that task. Following the natural hypothesis explained in section \ref{Natural-Hypothesis}, the source code is treated like a document written in a natural language, just like the ones that have been successfully modelled with and treats source code similarly to documents written in natural language, an area in which RNNs and especially LSTMs have yielded impressive results. They are specifically tailored to deal with sequential data and have also been used for program analysis before. This work leverages Long Short Term Memory Networks (LSTM) to capture the context relationships in source code. 
%TODO continue here



\newpage
\subsection{Evaluation}
\subsubsection{Quantitative Evaluation}
For the purpose of prediction and classification, four key concepts are usually the basis for evaluation: true positives, true negatives, false positive and false negatives. They have been mentioned before but shall be defined properly here. Positive and negative refer to the prediction, meaning that (without loss of generality) in this work a prediction of 'vulnerable' would be a positive and a prediction of 'not vulnerable' would be a negative. The terms true and false refer to whether the prediction corresponds to the actual value or external judgement. Hence, a false positive is a clean code incorrectly labeled as vulnerable by the classifier, a true positive is a vulnerability that was correctly spotted, a false negative is an actual vulnerability that was not classified as such, and a true negative is a piece of code that was classified as 'not vulnerable' and is indeed harmless.\\
Two measurements metrics are directly derived from those four values: precision and recall. The \textbf{precision} is the rate of true positives within all positives. It measure how precise the model is in terms of how many of the predicted positives are actual positives, or phrased differently, how much trust can be placed in the classification of a positive and how many false alarms are produced. The \textbf{recall}, also called sensitivity, is a measurement for the rate of positives that were correctly identified in comparision to the total number of actual positives. One could take it as a measurement for how vigilantly the classifier spots all positives - or how much gets overlooked.\newline
\mbox{}\newline
$precision = \frac{true~positives}{true~positives~+~false~positives}$\newline
\mbox{}\newline
$recall = \frac{true~positives}{true~positives~+~false~negatives}$\newline
\mbox{}\newline
The \textbf{Accuracy} is the fraction of correct predictions compared to all predictions. For binary classification, it is defined as following:  \newline
\mbox{}\newline
$accuracy = \frac{true~positives~+~true~negatives}{true~positives~+~true~negatives~+~false~positives~+~false~negatives}$\newline
\mbox{}\newline
However, accuracy does not provide much insight when there is a class imbalanced data set, meaning that there are many more positives than negatives or vice versa. In the case of vulnerability detection, it is indeed the case that most code fragments will be clean and vulnerabilities are relatively rare. For example, ~\cite{Morrison.2015} found that their dataset of Windows code contained only 0.003\% vulnerable files, and \cite{Shin.2010} report that 3\% of their files in Mozilla Firefox had vulnerabilities. In a case where true positives are rare and true negatives are very common, a classifier can achieve a high accuracy score even though it misses most of the positives, as the many true negatives make it seem like the overall outcome was quite accurate. Therefore, the accuracy alone is not a suitable measurement for this application.\\
The \textbf{F1 score} is a balanced score (harmonic mean) that takes precision and recall into account. The F1 score is not as easily influenced by a large number of true negatives and is better suited for class-imbalanced data sets. The F1 score is defined as following:\\
\mbox{}\newline
$F1 = 2 \cdot \frac{precision \cdot recall}{precision + recall}$
\mbox{}\newline
In an ideal, perfect case, the model would achieve a near 0\% rate for false positives and false negatives, meaning that precision and recall both are close to 1, as well as accuracy and F1 score. In this work, the rate of false positives, false negatives, the accuracy and the F1 score will be used to evaluate the model, even though many other works on similar topics only take the first three of those four values into account.\\
According to some researchers ~\cite{Morrison.2015,Shin.2013,Neuhaus.2007}, precision and recall values of 70\% are reasonable for prediction models, but modern approaches have shown some more impressive results, as has been described in section \ref{Related-Work}. A precision and recall of above 90\% seems like a desireable goal for this work.

\subsubsection{Quantitative Evaluation}
In the qualitative evaluation, the practical merits of the trained classifier are demonstrated by identifying new vulnerabilities that are fixed in projects. 
%TODO ausführlicher

\newpage
\section{Implementation}
In the following section, the technical details, design decisions, challenges and obstacles of this work are described in detail.

\subsection{Scraping Commits}
The first step of the work is to gather the necessary data from Github. At first, a relatively long list of keywords were used that are relevant to security. Those keywords are in part inspired by similar research ~\cite{Zhou.2017}, and also stem from the OWASP foundation's list of security threats ~\cite{OWASPFoundation.}. A python script using the requests library was created to collect the data, accessing the github API. 

\lstset{basicstyle=\small}
\begin{lstlisting}
"denial of service", "XXE","vuln","CVE","XSS","NVD","malicious","cross site","exploit","directory traversal","rce","remote code execution","XSRF","cross site request forgery","click jack","clickjack","session fixation","dom injection","cross origin","infinite loop","xpath injection","brute force","buffer overflow","cache overflow","command injection","cross frame scripting","csv injection","eval injection","execution after redirect","format string","path disclosure","function injection","replay attack","session hijacking","smurf","sql injection","flooding","tampering","sanitize","sanitise"
\end{lstlisting}

This set is combined with a second set of keywords related to improvements, fixes or changes, in such a manner that every possible combination of an element of the first and an element of the second set is taken into account. Since the second set contains keywords that indicate a problem or a fix, the combinaions should be helpful (although not sufficient) to distinguish actual security fixes from many other mentions of vulnerabilities, e.g. for illustrative purposes in showcase projects.

\begin{lstlisting}
"prevent", "fix", "attack", "protect", "issue", "correct", "update", "improve", "change", "check", "malicious", "insecure", "vulnerable", "vulnerability"
\end{lstlisting}

After combining every keyword from the first set with every keyword from the second set, for each of the combinations, a search request is sent to github. Executing all those queries took roughly 80 hours and resulted in a 354.230 commits from 105.403 repositories.

%TODO alle einfügen
 

\subsection{Filtering the Data}

%2 Filter Showcases
\paragraph{Filtering showcase and educational projects}\mbox{}\\
Repositories that are built specifically to demonstrate vulnerabilities are removed, since only non-artificial, actual fixes of problems are of interest for this work. Therefore, all repositories whose name contains one of the following keywords from our collection from the collection.

\begin{lstlisting}
"offensive", "pentest", "vulnerab", "security", "hack", "exploit"
\end{lstlisting}

Furthermore, all readme.MD files that are provided were queried, and checked to see if they contain any of the following more specific keywords:

\begin{lstlisting}
"offensive security", "pentest", "exploits", "vulnerability research", "hacking", "security framework", "vulnerability database", "simulated attack", "security research"
\end{lstlisting}

In this step, 2361 repositories were removed.

\paragraph{Filtering repositories without python code}\mbox{}\\
The next step is to remove repositories that don't contain any python code. For that, the url \lstinline[columns=fixed]{https://api.github.com/repos/repository/languages}, where \lstinline[columns=fixed]{repository} stands for the name of the repository, yields a reply from which the programming languages used in the repository can be determined. 64559 repositories can be discarded now now because they don't contain any python at all, leaving 45306 repositories.

\subsection{Collecting the Code Snippets}
\paragraph{Acquiring diff files}\mbox{}\\
%4 Download Everything
The next step is to run through the list of commits and retrieve all associated diff files (see section \ref{diff}). The diff contains the changes that were made in the commit in plain text, including file names and line numbers. The file names are used to determine if the commit we are looking at changed any python files, if not, the commit is taken out of the database.

At this point, the tool pydriller~\cite{Spadini.2018} was also considered to mine code, as it helps to automatically pull and search through commits. However, a simple http request to the diff file for every commit in the database already keeps our scripts running for days. Therefore, doing a full clone of all git repositories is far out of the question, as it would take longer than the timeframe of this whole work to complete.

%Under closer inspection of the data, it turned ot that some keywords were actually a poor decision. While for some, only very few  results could be gathered, others (like 'dos' and 'format string') often occur in completely different contexts that are not related to security or yielded results of mediocre quality ('NVD', 'CVE', 'vuln' etc.). Therefore, the data was later filtered to only include the most worthwile keywords from the first list:

%\begin{lstlisting}
%["sql injection", "command injection", "replay attack" ,"cross site", "XSRF", "remote code execution", "tampering"]
%\end{lstlisting}

\paragraph{Reconstructing the code snippets and extracting changes}\mbox{}\\
%20 make into dataset 
Having gathered the diff files, the code of the snippet before and after the change is reconstructed. A commit consists of one or several changes to files, and each of them is processed seperately. The simple structure of a diff makes this easy - the symbol @@ marks a new file change. The diff yields the lines that were changed, and three lines before and after the changed ones.  The next step is to reconstruct the state the code was in before and after the change by picking the lines that were added and are marked with a '+' for the 'after' version, and the lines that were removed and are marked with a '-' for the 'previous' version. 

%We can at this point identify python built-ins and python keywords, but should we?

%We only keep commits that have less than 7 changes in total, since we rely on the commit messsage to be relevant to the changes, and for a bulk commit this isn't neccessarily true. We are now saving all the changes, and our database is complete.


\subsection{Word2Vec embedding}
%python for training
To encode the code tokens in a word2vec vector, a fitting word2vec model is required that has been trained on python source code. To achieve this, a different collection of code is gathered from github, taking files from the most influential python repositories. Note that those repositories are likely to contain few security vulnerabilities, as well as bugs in general. Ideally, the corpus of mined code should be a good example for clean, working Python code. Similarly to ~\cite{Bhoopchand.2016} and ~\cite{Allamanis.2013}, the underlying assumption here is the  heuristic that popular code projects are of good quality. Github offers two metrics to assess popularity of a repository: stars (highlights similar to bookmarks set by users) and forks (copies for further development and experimentation in a personal project). Choosing python repositories with a high number of stars and forks, the following selection of example repositories is gathered:

\begin{itemize}[noitemsep]
	\item https://github.com/numpy/numpy
	\item https://github.com/django/django
	\item https://github.com/scikit-learn/scikit-learn
	\item https://github.com/tensorflow/tensorflow
	\item https://github.com/keras-team/keras
	\item https://github.com/ansible/ansible
	\item https://github.com/TheAlgorithms/Python
	\item https://github.com/pallets/flask
	\item https://github.com/ytdl-org/youtube-dl
	\item https://github.com/pandas-dev/pandas
	\item https://github.com/scrapy/scrapy
	\item https://github.com/kennethreitz/requests
	\item https://github.com/home-assistant/home-assistant
	\item https://github.com/ageitgey/face\_recognition
\end{itemize}
With the tool pydriller~\cite{Spadini.2018}, the python files in those repositories can be downloaded. After stripping out comment lines, the files are simply concatenated to form a big corpos of python code. Using the gensim implementation for python, the Word2Vec model is then trained on the corpus, using a dimensionality of 100.

The results are promising: the following table shows some words and their most similar other tokens as determined by the word2vec model. A cosine similarity of '1' would mean that the two tokens are identical or complete synonyms, anything above 0.5 can be interpreted as some kind of similarity. 

\begin{center}
	\begin{tabular}{ |c|c|c|c| } 
		\hline
		\textbf{import} & from (0.681) & as (0.484) & \_\_all\_\_ (0.429)\\ 
		\textbf{true} & false (0.963) & none (0.659) & name (0.505)\\  
		\textbf{while} & break (0.641)  & if (0.553) & continue (0.488) \\
		\textbf{if} & elif (0.859)  & or (0.741) & and (0.740) \\
		\textbf{try} & else (0.661)  & finally (0.569) & except (0.514) \\
		\textbf{in} & is (0.529)  & == (0.525) & hasattr (0.495) \\
		\textbf{+} & += (0.636)  & - (0.571) & \% (0.561) \\
		\textbf{x} & y (0.771)  & z (0.676) & p (0.592) \\

		\hline
	\end{tabular}
\end{center}

The word2vec model was trained in two different ways: first with code stripped of all comments, but leaving variable names, strings etc. completely intact. Second, with comments stripped, but strings replaced with a general 'string' token. The latter yielded slightly better results and was therefore chosen as default method.

%23w2v1.py

\subsection{Preparing the Data for Classification}
The collected data is still in the format of vulnerable and non vulnerable code snippets. The snippets are converted into a list of tokens (e.g. 'if', 'init', '2.3' or '+') and each of the token is replaced with its vector representation according to the Word2Vec model. Each list of vectors is associated with their binary label, '0' meaning vulnerable and '1' meaning not vulnerable or unknown status.\\
70\% of the data is then randomly selected as a training set, 15\% are taken as a test set for validation, and 15\% are put aside for a final evaluation at the very end of the experiments. This is well in line with general practice in training neural networks as well as other works on similar tasks, for example, \cite{Russell.2018} split their dataset in 80\% training, 10\% validation and 10\% final test set, and \cite{Li.2018} used a split of 80-20 in train and test set. \\

The lists of vectors, each representing what was one code snippet, are truncated and padded to achieve an equal length of 200 vectors per datapoint.

\subsection{The Long Short Term Memory Network}

\subsubsection{Embedding Code in a Vector}
\begin{figure}[ht]
	\centering
	\includegraphics[width=\linewidth]{img/Architecture}
	\caption{architecture of the model}
	\label{fig:architecture}
\end{figure}

To implement the Long Short Term Memory network, the Keras library is used. A sequential model is created.\\
Since the goal is train a binary classifier, the last layer is a dense output layer with a single neuron and sigmoid activation function to predict 0 or 1 for the two classes (vulnerable and not vulnerable).\\
For training the model, the binary crossentropy is chosen. \\

%Li 2018 use Adamax
%Russel et al use Adam
%DAM 2017 use RMSprop
%LI and RUSSEL explain details about their parameters

A major consideration in training deep networks is the learning rate. Too small means the network won't learn quickly enough, too large means that the network won't deal well with a local minimum or an ill-conditioned region. With learning rate adaption, it is possible to modify the learning rate over the course of the learning process. One of the most popular algorithms for doing so is the Adam optimizer.\\
The Adam optimizer is an optimization algorithm for updating network weights that was published in 2014 \cite{Kingma.2014}
and is designed for deep neural networks and often achieves very good results fast. It computes individal learning rates for different parameters. According to the authors, it combines the advantages of the Adaptive Gradient Algorithm (Adagrad), which works well with sparse gradients~\cite{Duchi.2011}, and Root Mean Square Propagation (RMSprop), which adapts learning rates based on recent magnitudes of the gradients for the weight and performs well on on-line and non-stationary problems~\cite{Tieleman.2012}. The name is derived from 'adaptive moment estimation'. The Adam optimizer uses the first and second moment of the gradient, which are defined as the expected value of that variable to the power of one or two, respectively - those are the mean and the centered variance of the gradient. To estimate them, Adam uses exponetially moving averages, with two parameters beta1 and beta2 which control the decay rates of those moving averages. \\
For the purpose of this project, different optimizers were compared, but Adam was best. %todo


%\subsubsection{Preventing Overfitting}
%Dropout layer


%\subsubsection{our model architecture}


% The feature vector can then be used for measuring similarity between functions, predicting defects, checking for duplicates or, as in this case, to predict vulnerabilities~\cite{Dam.2017}.



\newpage

\section{Results}



\newpage
\section{Discussion}

\subsection{Limitations and Threats to Validity}

\paragraph{Labeling based on the commit context}\mbox{}\\
A major threat to the external validity of this work is the reliance on commit contexts for classifying code snippets into vulnerable or (probably) not vulnerable. It is assumed that in general, there was an actual vulnerability, and the fixed version is in some way better than the previous versions. Of course, there are many situations in which those assumptions don't hold true, and as there is no other method of double-checking the vulnerability status applied in this work. The typical way do to this would be to chose a static analysis tool or bug checker, however, this work aims to work out the insights that can be gained without \textit{any} prior knowledge or pre-made assumptions about vulnerabilities, just using the code database a a source of information for the neural network. There might be a lot of undetected vulnerabilities in the files labeled as 'possibly not vulnerable' just because no developer ever fixed them, and our network can therefore also not learn to spot them. Furthermore, some fixes could be plain wrong. The answer to this problem is purely the size of the dataset, in which such retrictions might not play a big role compared to many legitimate fixes that share characteristics.
\paragraph{Systematic oversights due to developer decisions}\mbox{}\\
The dataset does only contain fixes that were actually applied by developers, and vulnerabilities that are either never detected or that developers chose to ignore remain invisible for the algorithm. This leads inevitably to blind spots in the classifier which retrict its ability to spot vulnerabilities. Note that \cite{Liu.2018} use exactly this restriction in their work \textit{on purpose}, as they are trying to learn which violations are actually fixed by developers and are therefore with some certainity not false positives. The situation is therefore ambivalent: false positives as they would result from using tools that report assumed violations are avoided better by only taking problems into account that were actually fixed. On the other hand, false negatives could be introduced for vulnerabilities that developers don't notice, understand or care about.
\paragraph{Limitations in types of vulnerabilities}\mbox{}\\
Altough a wide variety of different types of vulnerabilities was taken into account than in many other works, the list is of course not complete and the work focuses only on the most important and typical vulnerabilities. Therefore, rare and atypical kinds of vulnerabilities are not caught by the vulnerability detection.
\paragraph{Issues regarding the underlying data}\mbox{}\\
The data collected from Github might not be entirely representative of python source code in general. All data stems from projects that are publicly available on the platform, therefore the findings might not be applicable to industry and closed-source projects that might have different approaches reagarding code quality and spotting vulnerabilities. Furthermore, although we excluded projects that were marked as a fork on Github, it is very well possible that several projects in our database are very similar to each other. By the nature of open source and code sharing practises, some parts of the code in different repositories might even come close to being duplicates, meaning that the dataset is more limited than it first seems. Threats to internal validity include the fact that only python files were taken into account and e.g. XML files were ignored, although they might include security-relevant information. 
\paragraph{Unknown vulnerabilities}\mbox{}\\
The predictions are evaluated against known vulnerabilities in the past. A prerequisite for the model is the existence of vulnerabilities to learn from, and in cases where no known examples for code with vulnerabilities are available, this method cannot be applied. As pointed out by ~\cite{Yamaguchi.2012}, such situations are rare in practise, as the main concern for large software repositories is not to discover a single vulnerability, but to make sure that the same type of error does not spread across the project, which is what this method is useful for. Future vulnerabilities that are not yet known, or that developers are not aware of yet, can of course not be taken into account.
\paragraph{Not capturing the full context of a vulnerability}\mbox{}\\
The diff around a change provides that changed lines and three lines before and after them, respectively. There are certainly many situations in which a vulnerability steams from the interaction of lines of code that are spread out over a large file (or several files) and are interacting despite their distance from each other. However, the examples for vulnerabilities used to train the model only focus on the immediate surroundings of the fixed lines, and therefore, the model cannot learn the implications of far-reaching dependencies. However, the immediate surroundings of around 400-500 code tokens are most important to determine whether a code snippet contains a vulnerability. % TODO argue
\paragraph{Conclusion Vulnerability}\mbox{}\\
By using standard performance measures for the classifier that have been applied in other works regarding vulnerability prediction, threat to conclusion validity are minimized. To evaluate the performance of the predictions, accuracy, precision, and recall were used. However, in practice there may be other metrics and representation demonstrating how well a classifier performs.
\paragraph{Chosen method, model and parameters}\mbox{}\\
Threats to internal validity also include limitations in the chosen method. The basic prediction model used in this work is a LSTM, and as described at length in section \ref{Related-Work}, other models are used as well, such as CNNs, Naive Bayes and other. The results obtained from different models may vary. The data has been embedded using a Word2Vec model that was custom made for this application, and shortcomings in the Word2Vec model could affect the overall performance of the LSTM. Furthermore, some design decisions were, such as excluding occurrences of more than three seperators in a row, which might effect the overall end result.  
\paragraph{Relying on source code}\mbox{}\\
The present design of this work is limited to dealing with vulnerabilities in source code, for which such source code must be available. The detection of vulnerabilities in binary files or executables is a different problem that is not tackled here. 
\paragraph{Limitation in programming language}\mbox{}\\
The present version of the Word2Vec encoding used, the collection of files for training and testing the work, and the whole concept only focus on python code. While there is no specific reason why python should be more or less suitable for vulnerability detection than other languages that researchers in the field focus on (mainly C, C++, Java, PHP and Javascsript), this is undoubtedly a strong restriction of the model.


\subsection{Future Work}




\section{Conclusion}



\section{Bibliography}

\bibliographystyle{alpha}
\bibliography{bibliography,architectureopt}



\cleardoublepage% Wieder auf eine eigene Doppelseite
{\parindent0cm
	\subsection*{Selbständigkeitserklärung}
	Ich erkläre hiermit, dass ich die vorliegende Arbeit selbständig verfasst
	und noch nicht für andere Prüfungen eingereicht habe.
	Sämtliche Quellen einschließlich Internetquellen, die unverändert oder
	abgewandelt wiedergegeben werden, insbesondere Quellen für Texte, Grafiken,
	Tabellen und Bilder, sind als solche kenntlich gemacht. Mir ist bekannt,
	dass bei Verstößen gegen diese Grundsätze ein Verfahren wegen
	Täuschungsversuchs bzw. Täuschung eingeleitet wird.
	\vspace{3\baselineskip}
	
	%		\vspace{3\baselineskip}
	%
	% 		\selectlanguage{english}
	% 		\subsection*{Statement of authorship}
	% 		Hier würde die englische Selbständigkeitserklärung folgen, falls gewünscht. Doch es fehlt eine akzeptable Übersetzung.
	% 		\vspace{3\baselineskip}
	%
	% 		Berlin, #2 \hfill \TitelPunktLinie{6cm}
}




\end{document}


%note that...



%Due to the fundamental inability of one program to completely analyze another program's code, a generic technique for finding arbitrary vulnerabilities does not exist [10]. As a consequence, all practical approaches either limit the search to specific types of vulnerabilities or, as in the case of vulnerability extrapolation, only identify potentially vulnerable code.~\cite{Yamaguchi.2012}

%Unfortunately, the process of finding vulnerabilities cannot be automated in the general case. According to Rice's theorem a computer program is unable to generally decide whether another program contains vulnerable code~\cite{Yamaguchi.2012}
